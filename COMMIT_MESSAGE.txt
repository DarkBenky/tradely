refactor: Simplify reward function to focus on actual profit maximization

BREAKING CHANGE: Reward function completely redesigned

Summary:
--------
Replaced complex 6-component reward system with simple profit-focused design.
Changed lookforward from 30 steps to 1 step for realistic short-term trading.

Changes:
--------
- portfolio_env.py: 
  * Rewrote _calculate_future_profit() to look only 1 step ahead
  * Simplified _calculate_reward() from ~150 lines to ~45 lines
  * Removed complex multi-objective optimization
  * New formula: portfolio_weighted_return * 1000 + outperformance_bonus * 500
  * Removed lookforward_window_size attribute
  * Updated step() bounds checking for 1-step lookahead
  * Added module docstring explaining reward philosophy

- synthetic_data_generator.py:
  * Changed default lookforward_steps from 20 to 1
  * Updated all instantiations to use lookforward_steps=1
  * Aligned synthetic data generation with new reward function

- New files:
  * test_reward_system.py - Verification tests
  * CHANGES_SUMMARY.txt - Detailed change documentation
  * REWARD_SYSTEM_REFERENCE.py - Quick reference guide
  * REWARD_TRANSFORMATION_DIAGRAM.txt - Visual before/after

Statistics:
-----------
2 files changed, 76 insertions(+), 259 deletions(-)
- Removed ~183 lines of complex reward logic
- Added clear, focused profit maximization

Benefits:
---------
✓ Clearer learning signal for agent
✓ Faster convergence expected
✓ Simpler to understand and debug
✓ Aligned with actual trading goals
✓ Natural cash penalty (opportunity cost)

Impact:
-------
- Models trained with old reward function will need retraining
- Synthetic training data should be regenerated
- Expect different but likely better agent behavior

Related: Fixes issue with overcomplicated reward function that was
         hindering agent learning and making debugging difficult.
